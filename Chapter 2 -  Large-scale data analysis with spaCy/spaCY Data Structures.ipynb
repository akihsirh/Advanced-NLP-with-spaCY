{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCY Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared vocab and StringStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy stores all shared data in a vocabulary, the Vocab.This includes words, but also the labels schemes for tags and entities. All strings are encoded to hash IDs to save memory. spaCy generates ID via a hash function. The string is stored only once in the string store which is available as nlp.vocab.strings.\n",
    "\n",
    "- String Store: It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
    "\n",
    "***Hash IDs can't be reversed, though. If a word in not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13539992551592322469\n",
      "headache\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a headache\")\n",
    "\n",
    "# Look up the hash for headache\n",
    "headache_hash = nlp.vocab.strings[\"headache\"]\n",
    "print(headache_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "headache = nlp.vocab.strings[headache_hash]\n",
    "print(headache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lexemes are context-independent entries in the vocabulary that can be looked up using a string or a hash ID in the vocab. They hold context-independent information about a word such as the text, or whether the the word consists of alphabetic characters.\n",
    "- Context dependent information like part-of-speech tags (POS), dependencies or entity labels are not contained within lexemes.\n",
    "- Like tokens, lexemes have and expose attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chai 11747502562491277758 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love masala chai\")\n",
    "lexeme = nlp.vocab['chai']\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)ß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of spaCY's central data structures.\n",
    "- Automatically created when text processed with nlp object.\n",
    "- But can be manually instantiated too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated wonder\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "# Automatic instantiation\n",
    "auto_doc = nlp(\"Automated wonder\")\n",
    "print(auto_doc.text)\n",
    "print(type(auto_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreary manual labor !\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "#Manual instantiation\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "#create list of words for doc\n",
    "man_words = ['Dreary','manual','labor','!']\n",
    "#create list of spaces for sentence\n",
    "man_spaces = [True,True,True,False]\n",
    "\n",
    "#instantiate doc\n",
    "man_doc = Doc(nlp.vocab, words=man_words, spaces=man_spaces)\n",
    "\n",
    "print(man_doc.text)\n",
    "print(type(man_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Span Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slice of a Doc consisting of one or more tokens.\n",
    "- Span takes at least three arguments: \n",
    "    - The doc it refers to\n",
    "    - Span start index\n",
    "    - Span end index (exclusive)\n",
    "    \n",
    "A Span is automatically created when a doc is automatically instantiated. To manually create one see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No entities in manual document\n"
     ]
    }
   ],
   "source": [
    "#import span class from tokens\n",
    "from spacy.tokens import Span\n",
    "\n",
    "#create new manual doc\n",
    "man_words2 = ['Still','doing','things','manually!']\n",
    "#create list of spaces for sentence\n",
    "man_spaces2 = [True,True,True,False]\n",
    "\n",
    "#instantiate doc\n",
    "man_doc2 = Doc(nlp.vocab, words=man_words, spaces=man_spaces)\n",
    "\n",
    "#create span \n",
    "man_span = Span(man_doc2, 0,3)\n",
    "\n",
    "#Create one with a label\n",
    "man_span_label = Span(man_doc2,0,3, label=\"Observation\")\n",
    "\n",
    "if len(man_doc2.ents) == 0:\n",
    "    print(\"No entities in manual document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreary manual labor 5897125384360097813\n"
     ]
    }
   ],
   "source": [
    "#Add span to doc's entity list\n",
    "man_doc2.ents = [man_span_label]\n",
    "\n",
    "for ent in man_doc2.ents:\n",
    "    print(ent.text, ent.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices\n",
    "- Doc and Span are very powerful and hold references and relationships of words and sentences\n",
    "- Convert result to strings as late as possible\n",
    "- Use token attributes if available – for example, token.i for the token index\n",
    "- Don't forget to pass in the shared vocab\n",
    "\n",
    "eg. Bad Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Anna\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Anna loved her sister Elsa\")\n",
    "\n",
    "# Get all tokens and POS tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check for proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if (index+1) < len(pos_tags) and pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a bad code because it only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Alexis\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Alexis soared high above the mountains\")\n",
    "\n",
    "#Use native token attributes\n",
    "for token in doc:\n",
    "    # Check for proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if (token.i+1) < len(doc) and doc[token.i + 1].pos_ == \"VERB\":\n",
    "            result = token.text\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
