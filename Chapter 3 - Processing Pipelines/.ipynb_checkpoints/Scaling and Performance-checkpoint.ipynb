{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to process a large volumes of text in spacy, the pipe method is helpful. This method processes texts as a stream and yields Doc objects. Performsnce wise it is much faster than looping through every text and calling nlp(text) on each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McDonalds is my favorite restaurant.\n",
      "['favorite']\n",
      "\n",
      "Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..\n",
      "['sick']\n",
      "\n",
      "People really still eat McDonalds :(\n",
      "[]\n",
      "\n",
      "The McDonalds in Spain has chicken wings. My heart is so happy \n",
      "['happy']\n",
      "\n",
      "@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P\n",
      "['delicious', 'fast']\n",
      "\n",
      "please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D\n",
      "['open', 'BAD']\n",
      "\n",
      "This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it\n",
      "['terrible']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"tweets.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for doc in list(nlp.pipe(TEXTS)):\n",
    "    print(doc.text)\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipe method also supports passing in tuples of text with context if you set \"as tuples\" to True. This is useful for passing in additional metadata for each Doc object. You can even add the context meta data to custom attributes. Output of this is (doc, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"bookquotes.json\") as f:\n",
    "    BOOK = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Doc metadata 'author'\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc metadata 'book' (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in list(nlp.pipe(BOOK, as_tuples=True)):\n",
    "    # Set book and author for each doc\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, \"\\n\", \"— '{}' by {}\".format(doc._.book, doc._.author), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also process text faster by running only the tokenizer on the text. This can happen in cases when you have methods that already perform some of the other components for eg a custom function that does parts of speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also temporarily and selectively disable pipeline components using the with clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
