{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you call nlp?\n",
    "\n",
    "1. The tokenizer is applied which turns the string of text into a Doc object. \n",
    "2. The Doc object passes through a series of pipeline components such as the tagger, then the parser and then the entity recognizer. \n",
    "3. Finally, the processed Doc is returned, so you can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the built-in pipeline components ?\n",
    "\n",
    "1. Tagger: Part-of-speech tagger -> sets token.tag attribute\n",
    "2. Parser: Dependency Parser -> sets token.dep and token.head attributes. It is responsible for detecting sentences and base noun phrases, also known as noun chunks\n",
    "3. Ner: Named entity recognizer -> adds the detected entities to the doc.ents property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "4. textcat: Text classifier sets category labels that apply to the whole text. This is added to the doc.cats property.\n",
    "\n",
    "**Text categories are very specific and are not included in the pre-trained models. However they can be trained when building your own system**\n",
    "\n",
    "All info about what components make up the pipeline is contained in the meta JSON. This JSON defines things like:\n",
    "1. Language and pipeline. \n",
    "2. Components to instantiate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods to access meta info\n",
    "- Names of the pipeline components -> nlp.pipe_names attribute.\n",
    "- List of component name and component function tuples -> nlp.pipeline attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x11d92d450>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x11d7899f0>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x11d789a60>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
